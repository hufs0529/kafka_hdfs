<br />
<div align="center">
  <a href="https://zenml.io">
    <img alt="spring" src="https://github.com/hufs0529/penterest/assets/81501114/7d3522e9-9e2a-4bbf-b06b-6748f23a84c6" alt="Logo" width="150" height="100">
    <img alt="kafka" src="https://github.com/hufs0529/kafka_hdfs/assets/81501114/c93a49a1-767a-4ba5-b7e9-1020f619dbc8" alt="Logo" width="150" height="100">
    <img alt="hdfs" src="https://github.com/hufs0529/kafka_hdfs/assets/81501114/3ca933e2-81a6-4d23-81b5-d987bfbbb5bb" alt="Logo" width="150" height="100">
    <img alt="docker-compose" src="https://github.com/hufs0529/kafka_hdfs/assets/81501114/82a79a6b-823e-4ada-a36d-2a98183c778d" alt="Logo" width="150" height="100">
    <img alt=s3" src="https://github.com/hufs0529/kafka_hdfs/assets/81501114/9127a01e-f3a0-48ca-a48d-d226595e6880" alt="Logo" width="150" height="100">


  </a>

<h3 align="center">Data Lake With Hadoop & KAFKA</h3>

  <h2 align="center">
    Springì˜ë¡œë¶€í„°ì˜ ì‚¬ìš©ì Log ìˆ˜ì§‘ ë° Kafka, Hadoopì„ ì´ìš©í•œ ë°ì´í„° ë ˆì´í¬
    <br />
    <br />
    <br />
  </h2>
  <h3>Role: Spring API, Kafka, Hadoop, S3</h3>

</div>

<br />

# ğŸ¤– API
  [Wanted Pre On Boarding ì‚¬ì „ ê³¼ì œ API ì´ìš©](https://github.com/hufs0529/penterest/wanted-pre-onboarding-backend)

# ğŸ¤¹ Introduction
-  logback-kafka-appender(https://github.com/danielwegener/logback-kafka-appender) ì„ ì´ìš©í•˜ì—¬ ìœ ì €ë¡œë¶€í„° Log ë°ì´í„° ìˆ˜ì§‘
-  docker-composeë¡œ êµ¬ì„±í•œ kafka ì„œë²„ë¥¼ í†µí•´ì„œ ë°ì´í„° ì£¼ì…
-  HDFS Sink connectorì„ í†µí•´ì„œ ë°ì´í„° HDFSì— ì ì¬


# ğŸ”‹ Architecture
<img width="527" alt="í™”ë©´ ìº¡ì²˜ 2023-08-22 133146" src="https://github.com/hufs0529/kafka_hdfs/assets/81501114/9870c107-a185-4874-ada1-f60f4e413b90">


# ğŸ—º docker-compose
        # kafka í´ëŸ¬ìŠ¤í„°ë§
        cd /kafka/docker compose up -d

        # hadoop í´ëŸ¬ìŠ¤í„°ë§
        cd /hadoop/docker compose up -d


# ğŸ‡ Composition
#### Kafka í´ëŸ¬ìŠ¤í„° ë° Kafka Connect, HDFS Sink Connector ì„¤ì •
- Kafka ì‹¤í–‰
  
       cd /kafka/docker compose up -d

- Kafka Connect ì„¤ì¹˜

        $ wget https://packages.confluent.io/archive/6.1/confluent-6.1.0.tar.gz
        $ tar xvf confluent-6.1.0.tar.gz
- Kafka Connect ì‹¤í–‰(Kafka ì„œë²„ê°€ ì‹¤í–‰ ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤)
        $ {KAFKA_HOME}/bin/connect-distributed -daemon ./etc/kafka/connect-distributed.properties

- Kafka Connect í™•ì¸
        $ ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
        <img width="527" alt="í™”ë©´ ìº¡ì²˜ 2023-08-22 133146" src="https://github.com/hufs0529/kafka_hdfs/assets/81501114/12f9a676-8847-4197-b20e-21db775a4c0b">
        - 4ê°œì˜ default í† í”½ ìƒì„±

- Confluent-Hub ì„¤ì¹˜
##### Confluent-Hubë¥¼ ì´ìš©í•˜ë©´ Connectorë¥¼ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆë‹¤
        # ëª…ë ¹ì–´ ì…ë ¥í•˜ë©´ confluent-hub ì••ì¶• íŒŒì¼ ë°›ì„ ìˆ˜ ìˆë‹¤
        curl -L --http1.1 -o confluent-hub-client-latest.tar.gz https://packages.confluent.io/archive/5.5/confluent-hub-client-5.5.3.tar.gz

        # ì••ì¶•í•´ì œ í›„ ë””ë ‰í† ë¦¬ ì…ì¥ í›„ /binë””ë ‰í† ë¦¬ ì…ì¥!!!
        # confluent-hub íŒŒì¼ ì´ ìˆëŠ”ë° docker commandì²˜ëŸ¼ /binë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥

        confluent-hub install confluentinc/kafka-connect-s3-source:latest

- HDFS Sink Connector ì„¤ì¹˜

  <img width="527" alt="í™”ë©´ ìº¡ì²˜ 2023-08-22 133146" src="https://github.com/hufs0529/kafka_hdfs/assets/81501114/3bb94563-87d9-485f-a4bb-277567f27a84">
##### /usr/share/confluent-hub-components PATHì— ë‹¤ìš´ë¡œë“œ í• ê±°ëƒê³  ë¬»ëŠ”ë° N ì…ë ¥ì‹œ ì§ì ‘ ì›í•˜ëŠ” ìœ„ì¹˜ì— ì„¤ì¹˜ ê°€ëŠ¥í•˜ë‹¤
##### ì£¼ì˜í•´ì•¼ í• ì ì€ /usr/share/confluent-hub-components ë””ë ‰í† ë¦¬ê°€ ìë™ ìƒì„±ì´ ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ mkdir /usr/share/confluent-hub-component í•„ìˆ˜!!!


- Config ì„¤ì •
  
            curl -X POST -H "Content-Type: application/json" --data '{
              "name": "hdfs3-sink",
              "config": {
                "connector.class": "io.confluent.connect.hdfs3.Hdfs3SinkConnector",
                "tasks.max": "1",
                "topics": "{TOPIC}",
                "hdfs.url": "hdfs://{namenode}:9000",
                "flush.size": "3",
                "key.converter": "org.apache.kafka.connect.storage.StringConverter",
                "value.converter": "io.confluent.connect.avro.AvroConverter",
                "value.converter.schema.registry.url":"http://localhost:8081",
                "confluent.topic.bootstrap.servers": "localhost:9092",
                "confluent.topic.replication.factor": "1"
              }
            }' http://localhost:8083/connectors
  ##### Kafka Connectorì€ APIê¸°ë°˜ìœ¼ë¡œ í†µì‹ í•˜ê¸° ë•Œë¬¸ì— POSTë¥¼ í†µí•´ì„œ Config ì„¤ì •ì´ ê°€ëŠ¥í•˜ë‹¤

### Spring ë‚´ Kafka Consumer ì„¤ì •

      # "exam" í† í”½ê³¼ "foo" Groupì— ëŒ€í•´ì„œ Kafka Consumer ë™ì‘
      public KafkaConsumer(HDFSConfig hdfsConfig) {
              this.hdfsConfig = hdfsConfig;
          }
      
          @KafkaListener(topics = "exam", groupId = "foo")
          public void consume(String message) throws Exception {
              System.out.println(String.format("Consumed message : %s", message));
      
              try {
                  writeMessageToS3(message);
              } catch (IOException e) {
                  e.printStackTrace();
              }
          }

    


#### Spring ë¡œê·¸ ì¶”ì¶œ
##### mavenì„¤ì •ì„ í†µí•´ì„œ ì˜ì¡´ì„± ì£¼ì…
        <dependency>
    			<groupId>com.github.danielwegener</groupId>
    			<artifactId>logback-kafka-appender</artifactId>
    			<version>0.1.0</version>
    		</dependency>
    		<dependency>
    			<groupId>net.logstash.logback</groupId>
    			<artifactId>logstash-logback-encoder</artifactId>
    			<version>5.2</version> <!-- Use the version you need -->
    		</dependency>
    		<dependency>
    			<groupId>ch.qos.logback</groupId>
    			<artifactId>logback-classic</artifactId>
    			<version>1.2.3</version>
    			<scope>runtime</scope>
    		</dependency>

##### resources/logback.xmlì„ í†µí•´ì„œ ë¡œê·¸ê°€ ì°í íŒ¨í„´ ë° topicì„ ì„¤ì •í•´ì¤€ë‹¤

        # logback.xml
        <?xml version="1.0" encoding="UTF-8"?>
          <configuration>
              <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
                  <layout class="ch.qos.logback.classic.PatternLayout">
                      <Pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</Pattern>
                  </layout>
              </appender>
          
              <!-- Layout 1  :  kafkaAppender -->
              <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
                  <encoder class="com.github.danielwegener.logback.kafka.encoding.LayoutKafkaMessageEncoder">
                      <layout class="ch.qos.logback.classic.PatternLayout">
                          <Pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</Pattern>
                          <!--                <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>-->
                      </layout>
                  </encoder>
                  <topic>{topic}</topic>
                  <producerConfig>bootstrap.servers=localhost:9092</producerConfig>
              </appender>
          
              <!-- Async í•œ KafkaAppender ì¶”ê°€ -->
              <appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
                  <appender-ref ref="kafkaAppender"/>
              </appender>
          
          
              <!-- logger -->
              <logger name="org.apache.kafka" level="ERROR"/>
              <logger name="com.minsub.java.logger.kafka" level="DEBUG">
                  <!-- Layout 1 -->
                  <appender-ref ref="kafkaAppender"/>
                  <!-- Layout 2 -->
                  <!--        <appender-ref ref="logstashKafkaAppender" />-->
              </logger>
          
              <root level="DEBUG">
                  <appender-ref ref="STDOUT"/>
              </root>
          
              <root level="info">
                  <appender-ref ref="ASYNC" />
              </root>

          
          </configuration>

##### LoggerFactory ì„¤ì •
        # ë¡œê·¸ê°€ ì°í Controllerì— ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •
        private static final Logger kafkaLogger = LoggerFactory.getLogger("kafkaLogger");
        

##### Log ì°ê¸°
        # ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ìê°€ ê²Œì‹œë¬¼ì„ ê²€ìƒ‰(search)í–ˆì„ ì‹œ ê´€ë ¨ ë¡œê·¸ê°€ ì°íˆëŠ” ê²ƒì„ í™œìš©í•˜ì˜€ë‹¤
        @ResponseStatus(HttpStatus.OK)
        @GetMapping("/search")
        public ResponseEntity search(
                @RequestParam(value = "sort", defaultValue = "createDate") String sort,
                @RequestParam(value = "page", defaultValue = "1") Integer page,
                @RequestParam(value = "size", defaultValue = "5") Integer size,
                @ModelAttribute PostSearchCondition postSearchCondition) {
            Pageable pageable = PageRequest.of(page - 1, size, Sort.by(sort).ascending());
    
            String searcher = checkWriter();
    
            kafkaLogger.info("Search request - Sort: {}, Page: {}, Size: {}, Condition: {}, Searcher: {}", sort, page, size, postSearchCondition, searcher);
    
            return ResponseEntity.ok(postService.getPostList(pageable, postSearchCondition));
        }


##### kafka consumer
![kafka log](https://github.com/hufs0529/kafka_hdfs/assets/81501114/eb82f20e-a854-40a2-8f47-139564c27015)



### S3 ì ì¬
##### ì•ì„  KafkaConsumer í´ë˜ìŠ¤ ë‚´ì— ì¡´ì¬í•˜ëŠ” s3 ì ì¬ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ìƒì„±ë˜ëŠ” ë‚ ì§œë³„ë¡œ s3ì— ì ì¬
        private void writeMessageToS3(String message) throws Exception {

            BasicAWSCredentials awsCreds = new BasicAWSCredentials("", "");
            AmazonS3 amazonS3 = AmazonS3ClientBuilder.standard()
                    .withRegion(Regions.AP_NORTHEAST_2)
                    .withCredentials(new AWSStaticCredentialsProvider(awsCreds))
                    .build();
    
            String s3Bucket = "{bucket}";
            String s3ObjectKey = generateFileName();// S3 ë²„í‚·ì— ì €ì¥ë  ê°ì²´ì˜ í‚¤ (ë””ë ‰í† ë¦¬ í¬í•¨)
    
            // ë©”ì‹œì§€ë¥¼ ë°”ì´íŠ¸ ë°°ì—´ë¡œ ë³€í™˜
            byte[] messageBytes = message.getBytes();
    
            // S3 ê°ì²´ ë©”íƒ€ë°ì´í„° ì„¤ì • (ì„ íƒ ì‚¬í•­)
            ObjectMetadata metadata = new ObjectMetadata();
            metadata.setContentLength(messageBytes.length);
            metadata.setContentType("text/plain"); // MIME íƒ€ì… ì„¤ì • (ì˜ˆ: í…ìŠ¤íŠ¸)
    
            // S3ì— ê°ì²´ ì—…ë¡œë“œ
            amazonS3.putObject(new PutObjectRequest(s3Bucket, s3ObjectKey, new ByteArrayInputStream(messageBytes), metadata));
        }
